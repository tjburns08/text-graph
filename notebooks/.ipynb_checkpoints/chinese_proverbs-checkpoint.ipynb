{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c2a5a2",
   "metadata": {},
   "source": [
    "This notebook will take in a list of questions and calculate sentence similarity scores between each of them using the BERT model. We will begin by importing the dataset itself below. Credit for the \"how-to\" and code below goes to this [fantastic article](https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1). The main instruction is that the column with the text must be called 'text' for this script to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "121eb5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dev = False\n",
    "dataset = 'chinese_proverbs' # https://www.kaggle.com/bryanb/scraping-sayings-and-proverbs\n",
    "column = 'text' # text\n",
    "model_name = 'all-mpnet-base-v2' #'dmis-lab/biobert-base-cased-v1.2' #'multi-qa-mpnet-base-dot-v1' # all-mpnet-base-v2\n",
    "cutoff = 0.6 # Note that cutoffs differ depending on the model used. 0.5 for all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe51a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>in_chinese</th>\n",
       "      <th>pin_yin</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>不作不死。</td>\n",
       "      <td>Bù zuò bù sǐ. 'Not do not die.'</td>\n",
       "      <td>If you don't do stupid things you won't end up in tragedy.</td>\n",
       "      <td>Wisdom</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>塞翁失马，焉知非福。</td>\n",
       "      <td>Sài Wēng shī mǎ, yān zhī fēi fú. 'Sai Weng [legendary old man's name] lost horse, how know not blessing'.</td>\n",
       "      <td>Blessings come in disguise.</td>\n",
       "      <td>Wisdom</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>小洞不补，大洞吃苦。</td>\n",
       "      <td>Xiǎodòng bù bǔ, dàdòng chī kǔ.'small hole not mend; big hole eat hardship'</td>\n",
       "      <td>If small holes aren't fixed, then big holes will bring hardship.</td>\n",
       "      <td>Wisdom</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>水满则溢。</td>\n",
       "      <td>Shuǐmǎn zé yì. 'water full but overflows'</td>\n",
       "      <td>Water flows in only to flow out.</td>\n",
       "      <td>Wisdom</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>读万卷书不如行万里路。</td>\n",
       "      <td>Dú wànjuànshū bù rú xíng wànlǐlù. 'reading 10,000 books, not as good as walking 10,000 li road'</td>\n",
       "      <td>It's better to walk thousands of miles than to read thousands of books.</td>\n",
       "      <td>Wisdom</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>龙潭虎穴。</td>\n",
       "      <td>Lóng tán hǔ xué. 'dragon pool tiger cave'</td>\n",
       "      <td>A dragon's pool and a tiger's den.</td>\n",
       "      <td>Dragons</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>画龙点睛。</td>\n",
       "      <td>Huàlóngdiǎnjīng. 'paint dragon dot eye'</td>\n",
       "      <td>Paint a dragon and dot the eye.</td>\n",
       "      <td>Dragons</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>叶公好龙。</td>\n",
       "      <td>Yè Gōng hào long.</td>\n",
       "      <td>Lord Ye loves dragons.</td>\n",
       "      <td>Dragons</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>125</td>\n",
       "      <td>鲤鱼跳龙门。</td>\n",
       "      <td>Lǐyú tiào lóng mén. 'carp jump dragon gate'</td>\n",
       "      <td>A carp has jumped the dragon's gate.</td>\n",
       "      <td>Dragons</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>强龙难压地头蛇。</td>\n",
       "      <td>Qiáng lóng nán yā dìtóu shé. 'strong dragon difficult suppress local snake'</td>\n",
       "      <td>Even a dragon struggles to control a snake in its native haunt.</td>\n",
       "      <td>Dragons</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     in_chinese  \\\n",
       "0             0          不作不死。   \n",
       "1             1     塞翁失马，焉知非福。   \n",
       "2             2     小洞不补，大洞吃苦。   \n",
       "3             3          水满则溢。   \n",
       "4             4   读万卷书不如行万里路。    \n",
       "..          ...            ...   \n",
       "122         122         龙潭虎穴。    \n",
       "123         123         画龙点睛。    \n",
       "124         124         叶公好龙。    \n",
       "125         125        鲤鱼跳龙门。    \n",
       "126         126      强龙难压地头蛇。    \n",
       "\n",
       "                                                                                                       pin_yin  \\\n",
       "0                                                                              Bù zuò bù sǐ. 'Not do not die.'   \n",
       "1    Sài Wēng shī mǎ, yān zhī fēi fú. 'Sai Weng [legendary old man's name] lost horse, how know not blessing'.   \n",
       "2                                   Xiǎodòng bù bǔ, dàdòng chī kǔ.'small hole not mend; big hole eat hardship'   \n",
       "3                                                                    Shuǐmǎn zé yì. 'water full but overflows'   \n",
       "4              Dú wànjuànshū bù rú xíng wànlǐlù. 'reading 10,000 books, not as good as walking 10,000 li road'   \n",
       "..                                                                                                         ...   \n",
       "122                                                                  Lóng tán hǔ xué. 'dragon pool tiger cave'   \n",
       "123                                                                    Huàlóngdiǎnjīng. 'paint dragon dot eye'   \n",
       "124                                                                                          Yè Gōng hào long.   \n",
       "125                                                                Lǐyú tiào lóng mén. 'carp jump dragon gate'   \n",
       "126                                Qiáng lóng nán yā dìtóu shé. 'strong dragon difficult suppress local snake'   \n",
       "\n",
       "                                                                         text  \\\n",
       "0                  If you don't do stupid things you won't end up in tragedy.   \n",
       "1                                                 Blessings come in disguise.   \n",
       "2            If small holes aren't fixed, then big holes will bring hardship.   \n",
       "3                                            Water flows in only to flow out.   \n",
       "4     It's better to walk thousands of miles than to read thousands of books.   \n",
       "..                                                                        ...   \n",
       "122                                        A dragon's pool and a tiger's den.   \n",
       "123                                           Paint a dragon and dot the eye.   \n",
       "124                                                    Lord Ye loves dragons.   \n",
       "125                                      A carp has jumped the dragon's gate.   \n",
       "126           Even a dragon struggles to control a snake in its native haunt.   \n",
       "\n",
       "    category   origin  \n",
       "0     Wisdom  Chinese  \n",
       "1     Wisdom  Chinese  \n",
       "2     Wisdom  Chinese  \n",
       "3     Wisdom  Chinese  \n",
       "4     Wisdom  Chinese  \n",
       "..       ...      ...  \n",
       "122  Dragons  Chinese  \n",
       "123  Dragons  Chinese  \n",
       "124  Dragons  Chinese  \n",
       "125  Dragons  Chinese  \n",
       "126  Dragons  Chinese  \n",
       "\n",
       "[127 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dat = pd.read_csv('../data/processed/' + dataset + '.csv')\n",
    "\n",
    "if dev == True:\n",
    "    dat = dat.head(100)\n",
    "    \n",
    "display(dat.shape)\n",
    "display(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eec6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional block where you filter and modify the dataset to fit your needs \n",
    "#d = {column: dat[column], 'links': dat[column + '_links']}\n",
    "#dat = pd.DataFrame(data=d)\n",
    "#display(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0fec1",
   "metadata": {},
   "source": [
    "Now we will isolate the questions as an array of sentences, which will be fed into a pre-trained user-specified model. We note that there was a \"module not found\" error in the code below. The maintainer of the sentence-transformers package fixed it and requires the user to install via \"pip install -U sentence-transformers.\" The code below takes a while to run, as the pre-trained model is quite large. A menu of models is [here](https://www.sbert.net/docs/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58005166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = dat[column].tolist()\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2bb064",
   "metadata": {},
   "source": [
    "TODO Here, we make a topic model for our sentence similarity embedding. We will use the BERTopic library, [here](https://maartengr.github.io/BERTopic/index.html#quick-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1571d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5dd2216",
   "metadata": {},
   "source": [
    "From here, we're going to use cosine similarity to determine which questions are most similar to each other. One example of this is below. We compare the initial question to the first five questions after the initial question. We will display the questions and the similarity scores and see if it makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e163cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "cos_sim = cosine_similarity(\n",
    "    [sentence_embeddings[20]],\n",
    "    sentence_embeddings\n",
    ").tolist()\n",
    "\n",
    "df = pd.DataFrame({'text': sentences,\n",
    "            'similarity': cos_sim[0]})\n",
    "\n",
    "# Arrange by similarity\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df.sort_values(by = 'similarity', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc30e9",
   "metadata": {},
   "source": [
    "We were going to run a UMAP on the vector space to get some intuition around what it looks like, but because UMAP has issues with python 3.9 at the moment, we're going to jump right to making a cosine similarity matrix that we will then turn into a graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "dist = sklearn.metrics.pairwise.cosine_distances(sentence_embeddings)\n",
    "dist\n",
    "\n",
    "# nn = sklearn.neighbors.kneighbors_graph(sentence_embeddings, n_neighbors = 1)\n",
    "# nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dist = np.where(dist == 0, 1, dist) # For the boolean below. Can't figure out compound boolean.\n",
    "\n",
    "g = ig.Graph.Adjacency(dist < cutoff) # Need to convert to boolean\n",
    "g = g.as_undirected()\n",
    "\n",
    "if dat.shape[0] <= 1000:\n",
    "    fig, ax = plt.subplots()\n",
    "    ig.plot(g, target=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27d4c9",
   "metadata": {},
   "source": [
    "Now we will do a quick measure of betweenness and eigenvector centrality to get a feel for what questions are the most central. We'll print the top 10 of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810100ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = g.degree()\n",
    "btw = g.betweenness()\n",
    "dat['degree'] = g.degree()\n",
    "dat['betweenness'] = g.betweenness()\n",
    "display(dat.sort_values(by='degree', ascending=False))\n",
    "display(dat.sort_values(by='betweenness', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b018de",
   "metadata": {},
   "source": [
    "From here, we run clustering to see if the the quesitons group into particular themes. We will use Louvain clustering, as it is often used in graph-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = ig.Graph.community_multilevel(g)\n",
    "#df = pd.DataFrame({'name': dat['Question'], 'cluster': clust.membership})\n",
    "dat['cluster'] = clust.membership\n",
    "\n",
    "# Get cluster ID, mine the per-cluster topics\n",
    "for i in pd.unique(dat['cluster']):\n",
    "    curr = dat[dat['cluster'] == i]\n",
    "    if curr.shape[0] > 5:\n",
    "        display(curr.sort_values(by='degree', ascending=False))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff97a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05aa948e",
   "metadata": {},
   "source": [
    "Now, we're going to pull the graph out as an edgelist and get a feel for who is connected to who. We will then export the edgelist for import into Neo4J. \n",
    "\n",
    "TODO 1) make this a searchable umap. 2) place this into neo4j 3) set up the virtual environemnt and stop installing anything to the regular environemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6473a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ddfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "el = g.get_edge_dataframe()\n",
    "el"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23cc074",
   "metadata": {},
   "source": [
    "Now we have to convert the edges from their edge IDs to the questions that are in the order of the IDs. The reason we're seeing numbers right now is that the original adjacency matrix was made with the IDs and not the questions themselves. We're going to do that using a simple conversion function. We're going to do this by creating a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069bbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dict = {}\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    q_dict[i] = sentences[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = [q_dict[i] for i in el['source']]\n",
    "e2 = [q_dict[i] for i in el['target']]\n",
    "\n",
    "el_df = pd.DataFrame({'edge1': e1, 'edge2': e2})\n",
    "el_df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "el_df.to_csv('../output/' + dataset + '_' + model_name + '_edgelist_dist_' + str(cutoff) + '.csv', encoding='utf-8-sig')\n",
    "dat.to_csv('../output/' + dataset + '_' + model_name + '_analyzed_' + str(cutoff) + '.csv', encoding='utf-8-sig')\n",
    "#model_out.to_csv('../output/' + dataset + '_topic_model' + str(cutoff) + '.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ee475",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2853f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d4e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
